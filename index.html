<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<link rel="shortcut icon" href="./pic/others/banban_icon.png">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Chongjian GE, CS, HKU, The University of Hong Kong">
<meta name="description" content="Chongjian GE&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
<title>Chongjian GE&#39;s Homepage</title>


<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>


<table class="header-container">
	<tbody>
		<tr>
			<td class="header-info">
				<div class="name-title">
					<h1>Chongjian GE <span class="chinese-name">葛崇剑</span></h1>
				</div>
				<div class="contact-links">
					<div class="contact-item">
						<img src="./pic/others/mail.jpg" alt="Email">
						<a href="mailto:rhettgee1995@gmail.com">rhettgee1995@gmail.com</a>
					</div>
					<div class="contact-item">
						<img src="./pic/others/google_scholar_logo.png" alt="Google Scholar">
						<a href="https://scholar.google.com/citations?user=Zusxnl8AAAAJ">Google Scholar</a>
					</div>
					<div class="contact-item">
						<img src="./pic/others/github_logo.png" alt="GitHub">
						<a href="https://github.com/ChongjianGE">Github</a>
					</div>
					<div class="contact-item">
						<img src="./pic/others/location.png" alt="Location">
						<a href="https://www.google.com/maps/place/San+Jose,+California,+USA">San Jose, California, USA</a>
					</div>
				</div>
			</td>
			<td class="profile-photo">
				<img src="./pic/banban.jpg" alt="Profile Photo" class="profile-image">
			</td>
		</tr>
	</tbody>
</table>


<h2>Biography </h2>
<p>
	I am a Research Scientist at <a href=https://research.adobe.com/>Adobe Research</a>, working on visual generation models. In 2024, I got my Ph.D. degree from the CS Department, the University of Hong Kong,
	under the supervision of <a href=http://luoping.me/>Prof. Ping Luo</a>. 
	I was also a visiting student at UC Berkeley, working with <a href=https://me.berkeley.edu/people/masayoshi-tomizuka/>Prof. Masayoshi Tomizuka</a>. We are actively hiring self-motivated interns. Please feel free to reach me if you are interested in interns at Adobe Research or collaborating with me.
</p>


<h2>News</h2>
<div style="height: 150px; overflow: auto; font-size: small;">
<ul>
	<li>
		[02/2025] Two papers was accepted by CVPR <span style="color:red; white-space: nowrap;"></span>.
	</li>
	<li>
		[01/2024] Three papers was accepted by ICLR <span style="color:red; white-space: nowrap;">(One Spotlight)</span>.
	</li>
	<li>
		[12/2023] One paper was accepted by AAAI.
	</li>
	<li>
		[07/2023] One paper was accepted by ICCV.
	</li>
	<li>
		[02/2023] One paper was accepted by TIP.
	</li>
	<li>
		[01/2023] One paper was accepted by ICLR 2023.
	</li>
	<li>
		[09/2022] One paper was accepted by NeurIPS 2022 Track Datasets and Benchmarks.
	</li>
	<li>
		[09/2022] One paper was accepted by NeurIPS 2022 <span style="color:red; white-space: nowrap;">(Oral)</span>.
	</li>
	<li>
		[01/2022] Two papers were accepted by ICLR 2022 <span style="color:red; white-space: nowrap;">(One Oral paper, and one Spotlight paper)</span>.
	</li>
	<li>
		[10/2021] One paper was accepted by NeurIPS 2021.
	</li>
	<li>
		[07/2021] One paper was accepted by ICCV 2021.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
</ul>
</div>



<h2> Publications</h2>
<span style="color:red; white-space: nowrap; font-size: small;"><b>(* indicates equal contribution, + indicates corresponding author)</b></span>
<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td><center><video width="250" autoplay loop muted><source src="./pic/2025/goku.mp4" type="video/mp4"></video></center></td>
		<td>
			<font size="2"><b>Goku:</b> Flow Based Video Generative Foundation Models,
			<br>
			<i>Shoufa Chen*, <b>Chongjian Ge*</b>, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b> 2025		
			<br>
			[<a href='https://arxiv.org/abs/2502.04896'><b>paper</b></a>|<a href='https://saiyan-world.github.io/goku/'><b>project page</b></a>|<a href='https://github.com/Saiyan-World/goku'><b>github</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/compGS_arxiv.png"></center></td>
		<td>
			<font size="2">CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians,
			<br>
			<i><b>Chongjian Ge</b>, Chenfeng Xu, Yuanfeng Ji, Chensheng Peng, Masayoshi Tomizuka, Ping Luo, Mingyu Ding<span style="color:red; white-space: nowrap;">+</span>, Varun Jampani<span style="color:red; white-space: nowrap;">+</span>, Wei Zhan</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b> 2025	
			<br>
			[<a href='https://chongjiange.github.io/compgs.html'><b>project page</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/pixart_sigma.png"></center></td>
		<td>
			<font size="2">PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation,
			<br>
			<i>Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge</b><span style="color:red; white-space: nowrap;">*</span>, Enze Xie<span style="color:red; white-space: nowrap;">*</span>, Yue Wu<span style="color:red; white-space: nowrap;">*</span>, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li</i>
			<br>European Conference on Computer Vision (<b>ECCV</b>) 2024
			<br>
			[<a href='https://arxiv.org/abs/2403.04692'><b>paper</b></a>|<a href='https://github.com/PixArt-alpha/PixArt-sigma'><b>code</b></a>|<a href='https://pixart-alpha.github.io/PixArt-sigma-project/'><b>project page</b></a>|<a href='https://mp.weixin.qq.com/s/JcfVi0P4Db37McsfE6eXdg'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/arxiv_pixart.png"></center></td>
		<td>
			<font size="2">PIXART-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis,
			<br>
			<i>Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, Jincheng Yu<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Lewei Yao<span style="color:red; white-space: nowrap;">*</span>, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2024 (<b>Spotlight</b>)
			<br>
			[<a href='https://arxiv.org/abs/2310.00426'><b>paper</b></a>|<a href='https://github.com/PixArt-alpha/PixArt-alpha'><b>code</b></a>|<a href='http://pixart-alpha.github.io/'><b>project page</b></a>|<a href='https://www.jiqizhixin.com/articles/2023-10-18'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iccv_metabev.png"></center></td>
		<td>
			<font size="2">MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation,
			<br>
			<i><b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, Enze Xie, Zhongdao Wang, Lanqing Hong, Huchuan Lu, Zhenguo Li, Ping Luo</i>
			<br>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2023
			<br>
			[<a href='https://arxiv.org/abs/2304.09801'><b>paper</b></a>|<a href='https://chongjiange.github.io/metabev.html'><b>project page</b></a>|<a href='https://github.com/ChongjianGE/MetaBEV'><b>code</b></a>|<a href='https://www.youtube.com/watch?v=TiEQpYq77Xo'><b>demo</b></a>|<a href='https://mp.weixin.qq.com/s/yLdkwhRz1lk9ySAHh3HWew'><b>chinese media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iccv_groupmixformer.png"></center></td>
		<td>
			<font size="2">Advancing Vision Transformers with Group-Mix Attention,
			<br>
			<i><b>Chongjian Ge</b>, Xiaohan Ding, Zhan Tong, Li Yuan, Jiangliu Wang, Yibing Song, Ping Luo</i>
			<br>
			[<a href='https://arxiv.org/abs/2311.15157'><b>paper</b></a>|<a href='https://github.com/AILab-CVC/GroupMixFormer'><b>code</b></a>|<a href='https://chongjiange.github.io/gma'><b>project page</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/auto_bench.png"></center></td>
		<td>
			<font size="2">Large Language Models as Automated Aligners for benchmarking Vision-Language Models,
			<br>
			<i>Yuanfeng Ji<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2024 
			<br>
			[<a href='https://arxiv.org/abs/2311.14580'><b>paper</b></a>|<a href='xx'><b>code (coming soom)</b></a>|<a href='xx'><b>project page (coming soom)</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iclr23_snclr.png"></center></td>
		<td>
			<font size="2">Soft Neighbors Are Positive Supporters in Contrastive Visual Representation Learning,
			<br>
			<i><b>Chongjian Ge</b>, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, and Ping Luo</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2023
			<br>
			[<a href='https://arxiv.org/abs/2303.17142'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/SNCLR'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/nips2021_care.png"></center></td>
		<td>
			<font size="2">Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning,
			<br>
			<i><b>Chongjian Ge</b>, Youwei Liang, Yibing Song, Jianbo Jiao, Jue Wang, and Ping Luo</i>
			<br>Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2110.05340'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/CARE'><b>code</b></a>|<a href='https://mp.weixin.qq.com/s/oGS4XSjO29fHdDQXV1vyvg'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/cvpr2021_dcton.png"></center></td>
		<td>
			<font size="2">Disentangled Cycle Consistency for Highly-realistic Virtual Try-On,
			<br>
			<i><b>Chongjian Ge</b>, Yibing Song, Yuying Ge, Han Yang, Wei Liu, and Ping Luo</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.09479'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/DCTON'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/tip_neal.png"></center></td>
		<td>
			<font size="2">Rethinking Attentive Object Detection via Neural Attention Learning,
			<br>
			<i><b>Chongjian Ge</b>, Yibing Song, Chao Ma, Yuankai Qi and Ping Luo</i>
			<br>JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING (<b>TIP</b>) 
			<br>
			[<a href='https://ieeexplore.ieee.org/document/10186342'><b>paper</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/arxiv_adaptformer.png"></center></td>
		<td>
			<font size="2">AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,
			<br>
			<i>Shoufa Chen<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo</i>
			<br>Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022
			<br>
			[<a href='https://arxiv.org/pdf/2205.13535.pdf'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/AdaptFormer'><b>code</b></a>|<a href='http://www.shoufachen.com/adaptformer-page/'><b>project page</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/iclr2022_evit.png"></center></td>
		<td>
			<font size="2">Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations,
			<br>
			<i>Youwei Liang, <b>Chongjian Ge</b>, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2022 (<b>Spotlight</b>)
			<br>
			[<a href='https://arxiv.org/pdf/2202.07800.pdf'><b>paper</b></a>|<a href='https://github.com/youweiliang/evit'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/iclr2022_cyclemlp.png"></center></td>
		<td>
			<font size="2">CycleMLP: A MLP-like Architecture for Dense Prediction,
			<br>
			<i>Shoufa Chen, Enze Xie, <b>Chongjian Ge</b>, Runjian Chen, Ding Liang, and Ping Luo</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2022 (<b>Oral</b>)
			<br>
			[<a href='https://arxiv.org/abs/2107.10224'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/CycleMLP'><b>code</b></a>|<a href='https://mp.weixin.qq.com/s/2ajLnRtLGgKMQ8Egxhe2FQ'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/cyclemlp_pami.png"></center></td>
		<td>
			<font size="2">CycleMLP: A MLP-like Architecture for Dense Visual Predictions,
			<br>
			<i>Shoufa Chen, Enze Xie, <b>Chongjian Ge</b>, Runjian Chen, Ding Liang, and Ping Luo</i>
			<br>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2023 
			<br>
			[<a href='https://ieeexplore.ieee.org/abstract/document/10210694'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/CycleMLP'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/cvpr2021_pfafn.png"></center></td>
		<td>
			<font size="2">Parser-Free Virtual Try-on via Distilling Appearance Flows,
			<br>
			<i>Yuying Ge, Yibing Song, Ruimao Zhang, <b>Chongjian Ge</b>, Wei Liu, and Ping Luo</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.04559'><b>paper</b></a>|<a href='https://github.com/geyuying/PF-AFN'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/iccv2021_woo.png"></center></td>
		<td>
			<font size="2">Watch Only Once: An End-to-End Video Action Detection Framework,
			<br>
			<i>Shoufa Chen, Peize Sun, Enze Xie, <b>Chongjian Ge</b>, Jiannan Wu, Lan Ma, Jiajun Shen, and Ping Luo</i>
			<br>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2021
			<br>
			[<a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/WOO'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/deepaccident.png"></center></td>
		<td>
			<font size="2">DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving,
			<br>
			<i>Tianqi Wang, Sukmin Kim, Wenxuan Ji, Enze Xie, <b>Chongjian Ge</b>, Junsong Chen, Zhenguo Li, Ping Luo</i>
			<br>The Association for the Advancement of Artificial Intelligence (<b>AAAI</b>) 2023
			<br>
			[<a href='https://arxiv.org/pdf/2304.01168.pdf'><b>paper</b></a>|<a href='https://deepaccident.github.io'><b>homepage</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/iclr24_instructdet.png"></center></td>
		<td>
			<font size="2">InstructDET: Diversifying Referring Object Detection with Generalized Instructions,
			<br>
			<i>Ronghao Dang, Jiangyan Feng, Haodong Zhang, <b>Chongjian Ge,</b> Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2024 
			<br>
			[<a href='https://arxiv.org/pdf/2310.05136.pdf'><b>paper</b></a>|<a href='https://github.com/jyFengGoGo/InstructDet'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/nips_amos.png"></center></td>
		<td>
			<font size="2">AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation,
			<br>
			<i>Yuanfeng Ji, Haotian Bai, <b>Chongjian Ge</b>, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo</i>
			<br>Advances in Neural Information Processing Systems (<b>NeurIPS</b> 2022 Track Datasets and Benchmarks) (<b>Oral</b>)
			<br>
			[<a href='https://arxiv.org/pdf/2206.08023.pdf'><b>paper</b></a>|<a href='https://amos22.grand-challenge.org/'><b>project page</b></a>]
		</td>
	</tr>

	</tbody>
</table>


<h2><font>Honors and Awards</font></h2>
<div class="awards-container">
    <div class="award-card">
        <div class="award-year">2023</div>
        <div class="award-content">
            <div class="award-title">CVPR Outstanding Reviewer</div>
            <div class="award-desc">Recognition for exceptional contribution to peer review process</div>
        </div>
    </div>
    
    <div class="award-card">
        <div class="award-year">2020-2024</div>
        <div class="award-content">
            <div class="award-title">Hong Kong PhD Fellowship Scheme (HKPFS)</div>
            <div class="award-desc">Prestigious fellowship awarded by HKU for outstanding PhD candidates</div>
        </div>
    </div>
    
    <div class="award-card">
        <div class="award-year">2020-2021</div>
        <div class="award-content">
            <div class="award-title">YS and Christabel Lung Postgraduate Scholarship</div>
            <div class="award-desc">Merit-based scholarship from HKU</div>
        </div>
    </div>
    
</div>



<h2><font>Academic Service</font></h2>
<div class="service-container">
    <div class="service-card">
        <div class="service-header">
            <i class="fas fa-graduation-cap"></i>
            <h3>Conference Review</h3>
        </div>
        <div class="service-content">
            <div class="conference-group">
                <div class="conference-tier">Conferences</div>
                <div class="conference-list">
                    <span class="conference-item">CVPR</span>
                    <span class="conference-item">ICCV</span>
                    <span class="conference-item">ECCV</span>
                    <span class="conference-item">NeurIPS</span>
                    <span class="conference-item">ICML</span>
                    <span class="conference-item">ICLR</span>
					<span class="conference-item">SIGGRAPH</span>
                    <span class="conference-item">ACMMM</span>
                    <span class="conference-item">WACV</span>
                </div>
            </div>
        </div>
    </div>
    
    <div class="service-card">
        <div class="service-header">
            <i class="fas fa-book-open"></i>
            <h3>Journal Review</h3>
        </div>
        <div class="service-content">
            <div class="journal-list">
                <div class="journal-item">IEEE TPAMI</div>
                <div class="journal-item">IEEE TIV</div>
                <div class="journal-item">IEEE Access</div>
                <div class="journal-item">IEEE JBHI</div>
            </div>
        </div>
    </div>
</div>



<p><center><font>
<br>&copy; Chongjian GE | Last updated: Feb. 2025 | <a href="photography/index.html">.</a> </font></center>
</p>

</div>
</body>
</html>
