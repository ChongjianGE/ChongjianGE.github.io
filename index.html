<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<link rel="shortcut icon" href="./pic/others/banban_icon.png">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Chongjian GE, CS, HKU, The University of Hong Kong">
<meta name="description" content="Chongjian GE&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Chongjian GE&#39;s Homepage</title>


<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>


<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Chongjian GE <font face="Arial"> 葛崇剑 </font></h1></div>
				<p>
				<img src="./pic/others/mail.jpg" height="22px">  rhettgee1995@gmail.com<br>
				<br>
				<img src="./pic/others/google_scholar_logo.png" height="22px">  <a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AMpAcmR4EkucQUnJIY07lMruJ-qv6OJZVSK2VozPQ8MjlQ2tTZQ6dsgNzMlt-fLofd5wzl9_pdkQ7rBJbv94jg&user=Zusxnl8AAAAJ">Google Sholar</a><br>
				<br>
				<img src="./pic/others/github_logo.png" height="22px">  <a href="https://github.com/ChongjianGE">Github</a><br>
				<br>
				<img src="./pic/others/location.png" height="22px"> San Jose, California, USA<br>
				</p>
			</td>
			<td>
				<img src="./pic/banban.jpg" border="0" width="220"><br>
			</td>
		</tr>
	</tbody>
</table>


<h2>Biography </h2>
<p>
	I am a Research Scientist at <a href=https://research.adobe.com/>Adobe Research</a>, working on visual generation models. In 2024, I got my Ph.D. degree from the CS Department, the University of Hong Kong,
	under the co-supervision of <a href=http://luoping.me/>Prof. Ping Luo</a>. 
	I was also a visiting student at UC Berkeley, working with <a href=https://me.berkeley.edu/people/masayoshi-tomizuka/>Prof. Masayoshi Tomizuka</a>. We are actively hiring self-motivated interns. Please feel free to reach me if you are interested in interns at Adobe Research or collaborating with me.
</p>


<h2>News</h2>
<div style="height: 150px; overflow: auto; font-size: small;">
<ul>
	<li>
		[02/2025] Two papers was accepted by CVPR <span style="color:red; white-space: nowrap;"></span>.
	</li>
	<li>
		[01/2024] Three papers was accepted by ICLR <span style="color:red; white-space: nowrap;">(One Spotlight)</span>.
	</li>
	<li>
		[12/2023] One paper was accepted by AAAI.
	</li>
	<li>
		[07/2023] One paper was accepted by ICCV.
	</li>
	<li>
		[02/2023] One paper was accepted by TIP.
	</li>
	<li>
		[01/2023] One paper was accepted by ICLR 2023.
	</li>
	<li>
		[09/2022] One paper was accepted by NeurIPS 2022 Track Datasets and Benchmarks.
	</li>
	<li>
		[09/2022] One paper was accepted by NeurIPS 2022 <span style="color:red; white-space: nowrap;">(Oral)</span>.
	</li>
	<li>
		[01/2022] Two papers were accepted by ICLR 2022 <span style="color:red; white-space: nowrap;">(One Oral paper, and one Spotlight paper)</span>.
	</li>
	<li>
		[10/2021] One paper was accepted by NeurIPS 2021.
	</li>
	<li>
		[07/2021] One paper was accepted by ICCV 2021.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
</ul>
</div>



<h2> Publications</h2>
<span style="color:red; white-space: nowrap; font-size: small;"><b>(* indicates equal contribution, + indicates corresponding author)</b></span>
<table id="tbPublications" width="100%">
	<tbody>
	<tr>
		<td><center><video width="250" autoplay loop muted><source src="./pic/2025/goku.mp4" type="video/mp4"></video></center></td>
		<td>
			<font size="2"><b>Goku:</b> Flow Based Video Generative Foundation Models,
			<br>
			<i>Shoufa Chen*, <b>Chongjian Ge*</b>, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b> 2025		
			<br>
			[<a href='https://arxiv.org/abs/2502.04896'><b>paper</b></a>|<a href='https://saiyan-world.github.io/goku/'><b>project page</b></a>|<a href='https://github.com/Saiyan-World/goku'><b>github</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/compGS_arxiv.png"></center></td>
		<td>
			<font size="2">CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians,
			<br>
			<i><b>Chongjian Ge</b>, Chenfeng Xu, Yuanfeng Ji, Chensheng Peng, Masayoshi Tomizuka, Ping Luo, Mingyu Ding<span style="color:red; white-space: nowrap;">+</span>, Varun Jampani<span style="color:red; white-space: nowrap;">+</span>, Wei Zhan</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b> 2025	
			<br>
			[<a href='https://chongjiange.github.io/compgs.html'><b>project page</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/pixart_sigma.png"></center></td>
		<td>
			<font size="2">PixArt-Σ: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation,
			<br>
			<i>Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge</b><span style="color:red; white-space: nowrap;">*</span>, Enze Xie<span style="color:red; white-space: nowrap;">*</span>, Yue Wu<span style="color:red; white-space: nowrap;">*</span>, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li</i>
			<br>European Conference on Computer Vision (<b>ECCV</b>) 2024
			<br>
			[<a href='https://arxiv.org/abs/2403.04692'><b>paper</b></a>|<a href='https://github.com/PixArt-alpha/PixArt-sigma'><b>code</b></a>|<a href='https://pixart-alpha.github.io/PixArt-sigma-project/'><b>project page</b></a>|<a href='https://mp.weixin.qq.com/s/JcfVi0P4Db37McsfE6eXdg'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/arxiv_pixart.png"></center></td>
		<td>
			<font size="2">PIXART-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis,
			<br>
			<i>Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, Jincheng Yu<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Lewei Yao<span style="color:red; white-space: nowrap;">*</span>, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2024 (<b>Spotlight</b>)
			<br>
			[<a href='https://arxiv.org/abs/2310.00426'><b>paper</b></a>|<a href='https://github.com/PixArt-alpha/PixArt-alpha'><b>code</b></a>|<a href='http://pixart-alpha.github.io/'><b>project page</b></a>|<a href='https://www.jiqizhixin.com/articles/2023-10-18'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iccv_metabev.png"></center></td>
		<td>
			<font size="2">MetaBEV: Solving Sensor Failures for BEV Detection and Map Segmentation,
			<br>
			<i><b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, Enze Xie, Zhongdao Wang, Lanqing Hong, Huchuan Lu, Zhenguo Li, Ping Luo</i>
			<br>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2023
			<br>
			[<a href='https://arxiv.org/abs/2304.09801'><b>paper</b></a>|<a href='https://chongjiange.github.io/metabev.html'><b>project page</b></a>|<a href='https://github.com/ChongjianGE/MetaBEV'><b>code</b></a>|<a href='https://www.youtube.com/watch?v=TiEQpYq77Xo'><b>demo</b></a>|<a href='https://mp.weixin.qq.com/s/yLdkwhRz1lk9ySAHh3HWew'><b>chinese media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iccv_groupmixformer.png"></center></td>
		<td>
			<font size="2">Advancing Vision Transformers with Group-Mix Attention,
			<br>
			<i><b>Chongjian Ge</b>, Xiaohan Ding, Zhan Tong, Li Yuan, Jiangliu Wang, Yibing Song, Ping Luo</i>
			<br>
			[<a href='https://arxiv.org/abs/2311.15157'><b>paper</b></a>|<a href='https://github.com/AILab-CVC/GroupMixFormer'><b>code</b></a>|<a href='https://chongjiange.github.io/gma'><b>project page</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/auto_bench.png"></center></td>
		<td>
			<font size="2">Large Language Models as Automated Aligners for benchmarking Vision-Language Models,
			<br>
			<i>Yuanfeng Ji<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2024 
			<br>
			[<a href='https://arxiv.org/abs/2311.14580'><b>paper</b></a>|<a href='xx'><b>code (coming soom)</b></a>|<a href='xx'><b>project page (coming soom)</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iclr23_snclr.png"></center></td>
		<td>
			<font size="2">Soft Neighbors Are Positive Supporters in Contrastive Visual Representation Learning,
			<br>
			<i><b>Chongjian Ge</b>, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, and Ping Luo</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2023
			<br>
			[<a href='https://arxiv.org/abs/2303.17142'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/SNCLR'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/nips2021_care.png"></center></td>
		<td>
			<font size="2">Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning,
			<br>
			<i><b>Chongjian Ge</b>, Youwei Liang, Yibing Song, Jianbo Jiao, Jue Wang, and Ping Luo</i>
			<br>Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2110.05340'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/CARE'><b>code</b></a>|<a href='https://mp.weixin.qq.com/s/oGS4XSjO29fHdDQXV1vyvg'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/cvpr2021_dcton.png"></center></td>
		<td>
			<font size="2">Disentangled Cycle Consistency for Highly-realistic Virtual Try-On,
			<br>
			<i><b>Chongjian Ge</b>, Yibing Song, Yuying Ge, Han Yang, Wei Liu, and Ping Luo</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.09479'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/DCTON'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/tip_neal.png"></center></td>
		<td>
			<font size="2">Rethinking Attentive Object Detection via Neural Attention Learning,
			<br>
			<i><b>Chongjian Ge</b>, Yibing Song, Chao Ma, Yuankai Qi and Ping Luo</i>
			<br>JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING (<b>TIP</b>) 
			<br>
			[<a href='https://ieeexplore.ieee.org/document/10186342'><b>paper</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/arxiv_adaptformer.png"></center></td>
		<td>
			<font size="2">AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,
			<br>
			<i>Shoufa Chen<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo</i>
			<br>Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022
			<br>
			[<a href='https://arxiv.org/pdf/2205.13535.pdf'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/AdaptFormer'><b>code</b></a>|<a href='http://www.shoufachen.com/adaptformer-page/'><b>project page</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/iclr2022_evit.png"></center></td>
		<td>
			<font size="2">Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations,
			<br>
			<i>Youwei Liang, <b>Chongjian Ge</b>, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2022 (<b>Spotlight</b>)
			<br>
			[<a href='https://arxiv.org/pdf/2202.07800.pdf'><b>paper</b></a>|<a href='https://github.com/youweiliang/evit'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/iclr2022_cyclemlp.png"></center></td>
		<td>
			<font size="2">CycleMLP: A MLP-like Architecture for Dense Prediction,
			<br>
			<i>Shoufa Chen, Enze Xie, <b>Chongjian Ge</b>, Runjian Chen, Ding Liang, and Ping Luo</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2022 (<b>Oral</b>)
			<br>
			[<a href='https://arxiv.org/abs/2107.10224'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/CycleMLP'><b>code</b></a>|<a href='https://mp.weixin.qq.com/s/2ajLnRtLGgKMQ8Egxhe2FQ'><b>media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/cyclemlp_pami.png"></center></td>
		<td>
			<font size="2">CycleMLP: A MLP-like Architecture for Dense Visual Predictions,
			<br>
			<i>Shoufa Chen, Enze Xie, <b>Chongjian Ge</b>, Runjian Chen, Ding Liang, and Ping Luo</i>
			<br>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2023 
			<br>
			[<a href='https://ieeexplore.ieee.org/abstract/document/10210694'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/CycleMLP'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/cvpr2021_pfafn.png"></center></td>
		<td>
			<font size="2">Parser-Free Virtual Try-on via Distilling Appearance Flows,
			<br>
			<i>Yuying Ge, Yibing Song, Ruimao Zhang, <b>Chongjian Ge</b>, Wei Liu, and Ping Luo</i>
			<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.04559'><b>paper</b></a>|<a href='https://github.com/geyuying/PF-AFN'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2021/iccv2021_woo.png"></center></td>
		<td>
			<font size="2">Watch Only Once: An End-to-End Video Action Detection Framework,
			<br>
			<i>Shoufa Chen, Peize Sun, Enze Xie, <b>Chongjian Ge</b>, Jiannan Wu, Lan Ma, Jiajun Shen, and Ping Luo</i>
			<br>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2021
			<br>
			[<a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/WOO'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/deepaccident.png"></center></td>
		<td>
			<font size="2">DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving,
			<br>
			<i>Tianqi Wang, Sukmin Kim, Wenxuan Ji, Enze Xie, <b>Chongjian Ge</b>, Junsong Chen, Zhenguo Li, Ping Luo</i>
			<br>The Association for the Advancement of Artificial Intelligence (<b>AAAI</b>) 2023
			<br>
			[<a href='https://arxiv.org/pdf/2304.01168.pdf'><b>paper</b></a>|<a href='https://deepaccident.github.io'><b>homepage</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/iclr24_instructdet.png"></center></td>
		<td>
			<font size="2">InstructDET: Diversifying Referring Object Detection with Generalized Instructions,
			<br>
			<i>Ronghao Dang, Jiangyan Feng, Haodong Zhang, <b>Chongjian Ge,</b> Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song</i>
			<br>International Conference on Learning Representations (<b>ICLR</b>) 2024 
			<br>
			[<a href='https://arxiv.org/pdf/2310.05136.pdf'><b>paper</b></a>|<a href='https://github.com/jyFengGoGo/InstructDet'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2022/nips_amos.png"></center></td>
		<td>
			<font size="2">AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation,
			<br>
			<i>Yuanfeng Ji, Haotian Bai, <b>Chongjian Ge</b>, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo</i>
			<br>Advances in Neural Information Processing Systems (<b>NeurIPS</b> 2022 Track Datasets and Benchmarks) (<b>Oral</b>)
			<br>
			[<a href='https://arxiv.org/pdf/2206.08023.pdf'><b>paper</b></a>|<a href='https://amos22.grand-challenge.org/'><b>project page</b></a>]
		</td>
	</tr>

	</tbody>
</table>


<h2><font> Honors and Awards </font></h2>
<ul>
	<li>CVPR 2023 Outstanding Reviewers, 2023</li>
	<li>Hong Kong PhD Fellowship Scheme (HKPFS), HKU, 2020-2024</li>
	<li>YS and Christabel Lung Postgraduate Scholarship, HKU, 2020-2021</li>
	<li>Rhino-Bird Program, Tencent, 2020-2022</li>
</ul>



<h2><font> Academic Service </font></h2>
<ul>
    <li>
        <b>Conference Review:</b><br>
        SIGGRAPH<br>
        IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<br>
        IEEE International Conference on Computer Vision (ICCV)<br>
        European Conference on Computer Vision (ECCV)<br>
        ACM International Conference on Multimedia (ACMMM)<br>
        IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)<br>
        Neural Information Processing Systems (NeurIPS)<br>
        International Conference on Machine Learning (ICML)<br>
        International Conference on Learning Representations (ICLR)<br>
    </li>

    <li>
        <b>Journal Review:</b><br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence<br>
        IEEE Transactions on Intelligent Vehicles<br>
        IEEE Access<br>
        IEEE Journal of Biomedical and Health Informatics<br>
    </li>
</ul>



<p><center><font>
<br>&copy; Chongjian GE | Last updated: Dec. 2021 | <a href="photography/index.html">.</a> </font></center>
</p>

</div>
</body>
</html>
