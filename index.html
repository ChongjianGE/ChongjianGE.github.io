<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Chongjian GE, CS, HKU, The University of Hong Kong">
<meta name="description" content="Chongjian GE&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Chongjian GE&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Chongjian GE <font face="Arial">    葛崇剑 </font></h1></div>

				<h3>Ph.D. Candidate</h3>
				<p>
					Dept. of Computer Science <br>
					The University of Hong Kong <br>
					Pokfulam, Hong Kong<br>
					<br>
					Email: <a href="mailto:rhettgee@connect.hku.hk">rhettgee AT connect DOT hku DOT hk</a><br>
				</p>
				<p>
					<a href="https://github.com/ChongjianGE"><img src="./pic/others/github_logo.png" height="30px"></a>&nbsp;&nbsp;
					<!--<a href="https://scholar.google.com/citations?user=R8mtv14AAAAJ&hl=en&oi=sra"><img src="./pic/others/google_scholar_logo.png" height="30px"></a>&nbsp;&nbsp;-->
				</p>
				</p>
			</td>
			<td>
				<img src="./pic/ChongjianGE.jpg" border="0" width="220"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography </h2>
<p>
	I am a fourth-year (2020-now) Ph.D. student in the Department of Computer Science, the University of Hong Kong,
	under the co-supervision of <a href="http://luoping.me/">Prof. Ping Luo</a> and <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Prof. Wenping Wang</a>. 
	Besides, I am currently also a visiting student at UC Berkeley, working with <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Prof. Masayoshi Tomizuka</a>, 
	and <a href="http://zhanwei.site">Prof. Wei Zhan</a>, specially on AIGC-related projects.
	<br>
	<br>
	My previous research interest includes computer vision and machine learning. 
	I have done some works on <b>visual generation, efficient learning, image foundation models designing and finetuning, and 3D perception.</b> 
	<br>
	<br>
	I am on the job market currently. Please feel free to reach me if you are interested in my research.
	<br>

</p>



<h2>News</h2>
<ul>
	<li>
		[01/2024] Three papers was accepted by ICLR <span style="color:red; white-space: nowrap;">(One Spotlight)</span>.
	</li>
	<li>
		[12/2023] One paper was accepted by AAAI.
	</li>
	<li>
		[07/2023] One paper was accepted by ICCV.
	</li>
	<li>
		[02/2023] One paper was accepted by TIP.
	</li>
	<li>
		[01/2023] One paper was accepted by ICLR 2023.
	</li>
	<li>
		[09/2022] One paper was accepted by NeurIPS 2022 Track Datasets and Benchmarks.
	</li>
	<li>
		[09/2022] One paper was accepted by NeurIPS 2022 <span style="color:red; white-space: nowrap;">(Oral)</span>.
	</li>
	<li>
		[01/2022] Two papers were accepted by ICLR 2022 <span style="color:red; white-space: nowrap;">(One Oral paper, and one Spotlight paper)</span>.
	</li>
	<li>
		[10/2021] One paper was accepted by NeurIPS 2021.
	</li>
	<li>
		[07/2021] One paper was accepted by ICCV 2021.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
</ul>




<h2> Publications [<a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AMpAcmR4EkucQUnJIY07lMruJ-qv6OJZVSK2VozPQ8MjlQ2tTZQ6dsgNzMlt-fLofd5wzl9_pdkQ7rBJbv94jg&user=Zusxnl8AAAAJ">Google Scholar</a>]</h2>
	<br>      <span style="color:red; white-space: nowrap;"><b>(* indicates equal contribution)</b></span>
	<br>

<!--<h3 class="title is-6">Preprints</h3>-->

<!--	<table id="tbPublications" width="100%">-->
<!--	<tbody>-->
<!--	-->
<!--</tbody></table>-->

<h3 class="title is-6">Published Papers</h3>
<table id="tbPublications" width="100%">
	<tbody>
	<tr>
			<td><center><img width="250" src="./pic/2023/arxiv_pixart.png"></center></td>
			<td>
				<font size="2">PIXART-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis,
				<br>
					<i>Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, Jincheng Yu<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Lewei Yao<span style="color:red; white-space: nowrap;">*</span>, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li</i>
				<br>
				International Conference on Learning Representations (<b>ICLR</b>) 2024 (<b>Spotlight</b>)
				<br>
				[<a href='https://arxiv.org/abs/2310.00426'><b>paper</b></a>|<a href='https://github.com/PixArt-alpha/PixArt-alpha'><b>code</b></a>|<a href='http://pixart-alpha.github.io/'><b>project page</b></a>|<a href='https://www.jiqizhixin.com/articles/2023-10-18'><b>media report</b></a>]
			</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2023/iccv_metabev.png"></center></td>
		<td>
			<font size="2">MetaBEV: Solving Sensor Failures for BEV  Detection and Map Segmentation,
			<br>
				<i><b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Junsong Chen<span style="color:red; white-space: nowrap;">*</span>, Enze Xie, Zhongdao Wang, Lanqing Hong, Huchuan Lu, Zhenguo Li, Ping Luo</i>
			<br>
				IEEE/CVF International Conference on Computer Vision (ICCV) 2023
			<br>
			[<a href='https://arxiv.org/abs/2304.09801'><b>paper</b></a>|<a href='https://chongjiange.github.io/metabev.html'><b>project page</b></a>|<a href='https://github.com/ChongjianGE/MetaBEV'><b>code</b></a>|<a href='https://www.youtube.com/watch?v=TiEQpYq77Xo'><b>demo</b></a>|<a href='https://mp.weixin.qq.com/s/yLdkwhRz1lk9ySAHh3HWew'><b>chinese media report</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/iccv_groupmixformer.png"></center></td>
		<td>
			<font size="2">Advancing Vision Transformers with Group-Mix Attention,
			<br>
				<i><b>Chongjian Ge</b>, Xiaohan Ding, Zhan Tong, Li Yuan, Jiangliu Wang, Yibing Song, Ping Luo</i>
			<br>
					In Submission
			<br>
				[<a href='https://arxiv.org/abs/2311.15157'><b>paper</b></a>|<a href='https://github.com/AILab-CVC/GroupMixFormer'><b>code</b></a>|<a href='https://chongjiange.github.io/gma'><b>project page</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2023/auto_bench.png"></center></td>
		<td>
			<font size="2">Large Language Models as Automated Aligners for benchmarking Vision-Language Models,
			<br>
			<i></i>Yuanfeng Ji<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2024 
			<br>
				[<a href='https://arxiv.org/abs/2311.14580'><b>paper</b></a>|<a href='xx'><b>code (coming soom)</b></a>|<a href='xx'><b>project page (coming soom)</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2023/iclr23_snclr.png"></center></td>
		<td>
			<font size="2">Soft Neighbors Are Positive Supporters in Contrastive Visual Representation Learning,
			<br>
			<i><b>Chongjian Ge</b>, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, and Ping Luo</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2023
			<br>
			[<a href='https://arxiv.org/abs/2303.17142'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/SNCLR'><b>code</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2021/nips2021_care.png"></center></td>
		<td>
			<font size="2">Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning,
			<br>
			<i><b>Chongjian Ge</b>, Youwei Liang, Yibing Song, Jianbo Jiao, Jue Wang, and Ping Luo</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2110.05340'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/CARE'><b>code</b></a>|<a href='https://mp.weixin.qq.com/s/oGS4XSjO29fHdDQXV1vyvg'><b>media report</b></a>]
		</td>
	</tr>


	<tr>
		<td><center><img width="250" src="./pic/2021/cvpr2021_dcton.png"></center></td>
		<td>
			<font size="2">Disentangled Cycle Consistency for Highly-realistic Virtual Try-On,
			<br>
			<i><b>Chongjian Ge</b>, Yibing Song, Yuying Ge, Han Yang, Wei Liu, and Ping Luo</i>
			<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.09479'><b>paper</b></a>|<a href='https://github.com/ChongjianGE/DCTON'><b>code</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2023/tip_neal.png"></center></td>
		<td>
			<font size="2">Rethinking Attentive Object Detection via Neural Attention Learning,
			<br>
			<i><b>Chongjian Ge</b>, Yibing Song, Chao Ma, Yuankai Qi and Ping Luo</i>
			<br>
			JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING (<b>TIP</b>) 
			<br>
				[<a href='https://ieeexplore.ieee.org/document/10186342'><b>paper</b></a>]
		</td>
	</tr>




	<tr>
		<td><center><img width="250" src="./pic/2022/arxiv_adaptformer.png"></center></td>
		<td>
			<font size="2">AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition,
			<br>
			<i>Shoufa Chen<span style="color:red; white-space: nowrap;">*</span>, <b>Chongjian Ge<span style="color:red; white-space: nowrap;">*</span></b>, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo</i>
			<br>
			Advances in Neural Information Processing Systems (<b>NeurIPS</b>) 2022
			<br>
			[<a href='https://arxiv.org/pdf/2205.13535.pdf'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/AdaptFormer'><b>code</b></a>|<a href='http://www.shoufachen.com/adaptformer-page/'><b>project page</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2022/iclr2022_evit.png"></center></td>
		<td>
			<font size="2">Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations,
			<br>
			<i>Youwei Liang, <b>Chongjian Ge</b>, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2022 (<b>Spotlight</b>)
			<br>
			[<a href='https://arxiv.org/pdf/2202.07800.pdf'><b>paper</b></a>|<a href='https://github.com/youweiliang/evit'><b>code</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2022/iclr2022_cyclemlp.png"></center></td>
		<td>
			<font size="2">	CycleMLP: A MLP-like Architecture for Dense Prediction,
			<br>
			<i>Shoufa Chen, Enze Xie, <b>Chongjian Ge</b>, Runjian Chen, Ding Liang, and Ping Luo</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2022 (<b>Oral</b>)
			<br>
			[<a href='https://arxiv.org/abs/2107.10224'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/CycleMLP'><b>code</b></a>|<a href='https://mp.weixin.qq.com/s/2ajLnRtLGgKMQ8Egxhe2FQ'><b>media report</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2023/cyclemlp_pami.png"></center></td>
		<td>
			<font size="2">	CycleMLP: A MLP-like Architecture for Dense Visual Predictions,
			<br>
			<i>Shoufa Chen, Enze Xie, <b>Chongjian Ge</b>, Runjian Chen, Ding Liang, and Ping Luo</i>
			<br>
			EEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2023 
			<br>
			[<a href='https://ieeexplore.ieee.org/abstract/document/10210694'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/CycleMLP'><b>code</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2021/cvpr2021_pfafn.png"></center></td>
		<td>
			<font size="2">Parser-Free Virtual Try-on via Distilling Appearance Flows,
			<br>
			<i>Yuying Ge, Yibing Song, Ruimao Zhang, <b>Chongjian Ge</b>, Wei Liu, and Ping Luo</i>
			<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.04559'><b>paper</b></a>|<a href='https://github.com/geyuying/PF-AFN'><b>code</b></a>]
		</td>
	</tr>



	<tr>
		<td><center><img width="250" src="./pic/2021/iccv2021_woo.png"></center></td>
		<td>
			<font size="2">Watch Only Once: An End-to-End Video Action Detection Framework,
			<br>
			<i>Shoufa Chen, Peize Sun, Enze Xie, <b>Chongjian Ge</b>, Jiannan Wu, Lan Ma, Jiajun Shen, and Ping Luo</i>
			<br>
			IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2021
			<br>
			[<a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf'><b>paper</b></a>|<a href='https://github.com/ShoufaChen/WOO'><b>code</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2023/deepaccident.png"></center></td>
		<td>
			<font size="2">DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving,
			<br>
			<i>Tianqi Wang, Sukmin Kim, Wenxuan Ji, Enze Xie, <b>Chongjian Ge</b>, Junsong Chen, Zhenguo Li, Ping Luo</i>
			<br>
			The Association for the Advancement of Artificial Intelligence (<b>AAAI</b>) 2023
			<br>
			[<a href='https://arxiv.org/pdf/2304.01168.pdf'><b>paper</b></a>|<a href='https://deepaccident.github.io'><b>homepage</b></a>]
		</td>
	</tr>

	<tr>
		<td><center><img width="250" src="./pic/2024/iclr24_instructdet.png"></center></td>
		<td>
			<font size="2">InstructDET: Diversifying Referring Object Detection with Generalized Instructions,
			<br>
			<i>Ronghao Dang, Jiangyan Feng, Haodong Zhang, <b>Chongjian Ge,</b> Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song</i>
			<br>
			International Conference on Learning Representations (<b>ICLR</b>) 2024 
			<br>
			[<a href='https://arxiv.org/pdf/2310.05136.pdf'><b>paper</b></a>|<a href='https://github.com/jyFengGoGo/InstructDet'><b>code</b></a>]
		</td>
	</tr>


	<tr>
		<td><center><img width="250" src="./pic/2022/nips_amos.png"></center></td>
		<td>
			<font size="2">AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation,
			<br>
				<i>Yuanfeng Ji, Haotian Bai,  <b>Chongjian Ge</b>, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo</i>
			<br>
				Advances in Neural Information Processing Systems (<b>NeurIPS</b> 2022 Track Datasets and Benchmarks) (<b>Oral</b>)
			<br>
			[<a href='https://arxiv.org/pdf/2206.08023.pdf'><b>paper</b></a>|<a href='https://amos22.grand-challenge.org/'><b>project page</b></a>]
		</td>
	</tr>



</tbody></table>



<h2><font> Honors and Awards </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  CVPR 2023 Outstanding Reviewers, 2023 <br>
		  Hong Kong PhD Fellowship Scheme (HKPFS), HKU, 2020-2024 <br>
		  YS and Christabel Lung Postgraduate Scholarship, HKU, 2020-2021 <br>
		  Rhino-Bird Program, Tencent, 2020-2022 <br>
		  <!--
		  First-Class Scholarship, XJTU, 2018-2019 <br>
		  High-Voltage Scholarship, XJTU, 2017-2018 <br>
		  Pengkang Scholarship, XJTU, 2016-2017 <br>
		  Huichuan Scholarship, XJTU, 2015-2016 <br>
          Samsung Scholarship, XJTU, 2014-2015 <br>-->
	  </font> </p>
</ul>

<h2><font> Teaching </font></h2>
<ul>
    <li>
		From Human Vision to Machine Vision [Section 2A, 2020] [CCST9049]</br>
    </li>
    <li>
		Deep Learning [Section A, 2021] [DASC7606]</br>
    </li>
</ul>

<h2><font> Academic Service </font></h2>
<ul>
    <li>
		<b>Conference Review:</b></br>
	SIGGRAPH </br>
	IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </br>
	IEEE International Conference on Computer Vision (ICCV) </br>
	European Conference on Computer Vision (ECCV) </br>
	ACM International Conference on Multimedia (ACMMM) </br>
	IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) </br>
	Neural Information Processing Systems (NeurIPS) </br>
	International Conference on Machine Learning (ICML) </br>
	International Conference on Learning Representations (ICLR) </br>
    </li>

    <li>
        <b>Journal Review:</b></br>
    IEEE Transactions on Pattern Analysis and Machine Intelligence </br>
	IEEE Transactions on Intelligent Vehicles </br>
	IEEE Access </br>
	IEEE Journal of Biomedical and Health Informatics </br>
    </li>
</ul>

<!--
<p align=right>
	<a class="pull-right" href="#">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=6MHnf6x9T7o4yK48jgnnnSpwrnfCQ4_eJCdmT3syJjw&cl=ffffff&w=a"></script></center>
	</a>
</p>
-->



<p><center><font>
        <br>&copy; Chongjian GE | Last updated: Dec. 2021 | <a href="photography/index.html">.</a> </font></center>
</p>

</div>
</body></html>
