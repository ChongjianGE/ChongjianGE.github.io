<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MetaBEV">
  <meta name="keywords" content="SSL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Advancing Vision Transformers with Group-Mix Attention</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/metabev_images/car.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Advancing Vision Transformers with Group-Mix Attention</h1>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
               <a href="https://chongjiange.github.io/">Chongjian Ge</a><sup>1</sup>,</span>
            <span class="author-block">
               <a href="https://dingxiaohan.xyz">Xiaohan Ding</a><sup>2</sup>,</span>
            <span class="author-block">
               <a href="https://scholar.google.com/citations?user=6FsgWBMAAAAJ&hl=zh-CN">Zhan Tong</a><sup>3</sup>,</span>
            <span class="author-block">
               <a href="https://yuanli2333.github.io">Li Yuan</a><sup>4</sup>,</span>
            <span class="author-block">
               <a href="https://laura-wang.github.io">Jiangliu Wang</a><sup>5</sup>,</span>
            <span class="author-block">
               <a href="https://ybsong00.github.io">Yibing Song</a><sup>6</sup>,</span>
            <span class="author-block">
               <a href="http://luoping.me/">Ping Luo</a><sup>1+</sup></span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>2</sup>Tencent AI Lab,</span>
            <span class="author-block"><sup>3</sup>Ant Research,</span>
            <span class="author-block"><sup>4</sup>Peking University,</span>
            <span class="author-block"><sup>5</sup>The Chinese University of Hong Kong,</span>
            <span class="author-block"><sup>6</sup>AI3 Institute, Fudan University,</span>
          </div>



        </div>
      </div>
       <div class="column has-text-centered">
            <div class="publication-links">

              <span class="link-block">
                <a href="xxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AILab-CVC/GroupMixFormer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>
         </div>
    </div>
  </div>
</section>





<!--##################################   Abstract   ##################################-->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision Transformers (ViTs) have been shown to enhance visual recognition through modeling long-range dependencies with multi-head self-attention (MHSA), which is typically formulated as Query-Key-Value computation. However, the attention map generated from the Query and Key  captures only token-to-token correlations at one single granularity. In this paper, we argue that self-attention should have a more comprehensive mechanism to capture correlations among tokens and groups (i.e., multiple adjacent tokens) for higher representational capacity. Thereby, we propose Group-Mix Attention (GMA) as an advanced replacement for traditional self-attention, which can simultaneously capture token-to-token, token-to-group, and group-to-group correlations with various group sizes. To this end, GMA splits the Query, Key, and Value into segments uniformly and performs different group aggregations to generate group proxies. The attention map is computed based on the mixtures of tokens and group proxies and used to re-combine the tokens and groups in Value. Based on GMA, we introduce a powerful backbone, namely GroupMixFormer, which achieves state-of-the-art performance in image classification, object detection, and semantic segmentation with fewer parameters than existing models. For instance, GroupMixFormer-L (with 70.3M parameters and $\mathit{384}^2$ input) attains 86.2\% Top-1 accuracy on ImageNet-1K without external data, while GroupMixFormer-B (with 45.8M parameters) attains 51.2\% mIoU on ADE20K. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <h3 class="title">Acknowledgements</h3>
          <p>
            The website template was borrowed from <a href="https://nerfies.github.io/">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



<!--

</body>
</html>
